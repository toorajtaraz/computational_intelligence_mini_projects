#+TITLE: Unsupervised Learning
#+AUTHOR: Tooraj Taraz
#+DATE: Mon Oct 25 18:13:34 2021
#+OPTIONS: ^:nil _:nil
#+SETUPFILE: ~/.doom.d/retro-dark.theme
* K-MEANS
** How does it work?
K-Means algorithm initially picks K points randomly and starts forming clusters based on distance (assigning points to closest seed). Each iteration improves the overall quality of clusters and we stop when no change has occurred between two iteration.
** Loading dataset and needed libraries
Before we begin we need to import needed libraries which implement K-Means clustering algorithm, I've used two different implementation, One that utilizes CPU and one that utilizes GPU.
#+BEGIN_SRC python
from util import _x, _y_int #uses a few lines of code to load mnist dataset
from sklearn.cluster import MiniBatchKMeans #CPU accelerated kmeans
from sklearn.metrics import accuracy_score, adjusted_rand_score #to run metrics
import numpy as np #to work with numbers :))
from fast_pytorch_kmeans import KMeans #GPU accelerated kmeans
import torch #GPU accelerated kmeans
from tabulate import tabulate #for pretty output
#+END_SRC
Some of these libraries are going to be used for other algorithms as well, to avoid repetion I'll put comment for them only once.
Here is my util script (the loader function is not my implementation, I found it in an closed issue on Github, but I have modified it a little)
#+BEGIN_SRC python
import gzip
import os
from urllib.request import urlretrieve
import numpy as np
import matplotlib.pyplot as plt


def mnist(path=None):
    r"""Return (train_images, train_labels, test_images, test_labels).

    Args:
        path (str): Directory containing MNIST. Default is
            /home/USER/data/mnist or C:\Users\USER\data\mnist.
            Create if nonexistant. Download any missing files.

    Returns:
        Tuple of (train_images, train_labels, test_images, test_labels), each
            a matrix. Rows are examples. Columns of images are pixel values.
            Columns of labels are a onehot encoding of the correct class.
    """
    url = 'http://yann.lecun.com/exdb/mnist/'
    files = ['train-images-idx3-ubyte.gz',
             'train-labels-idx1-ubyte.gz',
             't10k-images-idx3-ubyte.gz',
             't10k-labels-idx1-ubyte.gz']

    if path is None:
        # Set path to /home/USER/data/mnist or C:\Users\USER\data\mnist
        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')

    # Create path if it doesn't exist
    os.makedirs(path, exist_ok=True)

    # Download any missing files
    for file in files:
        if file not in os.listdir(path):
            urlretrieve(url + file, os.path.join(path, file))
            print("Downloaded %s to %s" % (file, path))

    def _images(path):
        """Return images loaded locally."""
        with gzip.open(path) as f:
            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols
            pixels = np.frombuffer(f.read(), 'B', offset=16)
        return pixels.reshape(-1, 784).astype('float32') / 255

    def _labels(path):
        """Return labels loaded locally."""
        with gzip.open(path) as f:
            # First 8 bytes are magic_number, n_labels
            integer_labels = np.frombuffer(f.read(), 'B', offset=8)

        def _onehot(integer_labels):
            """Return matrix whose rows are onehot encodings of integers."""
            n_rows = len(integer_labels)
            n_cols = integer_labels.max() + 1
            onehot = np.zeros((n_rows, n_cols), dtype='uint8')
            onehot[np.arange(n_rows), integer_labels] = 1
            return onehot

        return _onehot(integer_labels), integer_labels

    train_images = _images(os.path.join(path, files[0]))
    train_labels, train_int_labels = _labels(os.path.join(path, files[1]))
    test_images = _images(os.path.join(path, files[2]))
    test_labels, test_int_labels = _labels(os.path.join(path, files[3]))

    return train_images, train_labels, test_images, test_labels, train_int_labels, test_int_labels
#importing dataset
_x, _y, _a, _b, _y_int, _b_int= mnist('/path/to/desired/folder')

#printing loaded data info
print(_y)
print(_x)
#used for plotting hand written numbers from dataset
def plot_some():
    num = 20
    images = _x[:num]
    labels = _y[:num]

    num_row = 4
    num_col = 5
    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))
    for i in range(num):
        ax = axes[i//num_col, i%num_col]
        ax.imshow(images[i].reshape(28,28), cmap='gray')
        ax.set_title('Label: {}'.format(labels[i]))
    plt.tight_layout()
    plt.show()
#+END_SRC
** Extracting labels (classifying clusters)
When K-Means's execution is over we have K clusters, first we need to find out which number has occured the most in each cluster and represent that cluster with that number. Let's say one cluster includes 1, 1, 1, 1, 2, 7, 7, 1 in this case we consider this cluster, 1. "classify_clusters" handles what I just explained:
#+BEGIN_SRC python
def classify_clusters(l1, l2):
    ref_labels = {}
    for i in range(len(np.unique(l1))):
        index = np.where(l1 == i,1,0)
        ref_labels[i] = np.bincount(l2[index==1]).argmax()
    decimal_labels = np.zeros(len(l1))
    for i in range(len(l1)):
        decimal_labels[i] = ref_labels[l1[i]]
    return decimal_labels
#+END_SRC

l1 is array of each cluster's label (kmeans_main.labels_), and l2 is array of actual labels.
It's note worthy that there are two global variables in my script:
1. kmeans_main
2. cluster_ids_x

They are populated by init_clustring_* function and they contain an object containing labels!
These functions init_clustrng_* don't need much elaboration, they just call kmean constructor in each library and populate global variables.

#+BEGIN_SRC python
def init_clustring_scikit(cluster_count=10):
    global kmeans_main
    kmeans_main = MiniBatchKMeans(n_clusters=cluster_count, verbose=False)
    kmeans_main.fit(_x)

def init_clustring_torch(cluster_count=10):
    global clusters_from_label, cluster_ids_x
    _kmeans = KMeans(n_clusters=cluster_count, mode='euclidean', verbose=1)
    x = torch.from_numpy(_x)
    cluster_ids_x = _kmeans.fit_predict(x)
#+END_SRC
** Evaluation
At last we have test_accuracy_* functions, they are responsible for calculating and printing purity and random index scores.

#+BEGIN_SRC python
def test_accuracy_scikit():
    global kmeans_main
    decimal_labels = classify_clusters(kmeans_main.labels_, _y_int)
    print("predicted labels:\t", decimal_labels[:16].astype('int'))
    print("true labels:\t\t",_y_int[:16])
    print(60 * '_')
    AP = accuracy_score(decimal_labels,_y_int)
    RI = adjusted_rand_score(decimal_labels,_y_int)
    print("Accuracy (PURITY):" , AP)
    print("Accuracy (RAND INDEX):" , RI)
    return AP, RI

def test_accuracy_torch():
    global cluster_ids_x
    decimal_labels = classify_clusters(cluster_ids_x.cpu().detach().numpy(), _y_int)
    print("predicted labels:\t", decimal_labels[:16].astype('int'))
    print("true labels:\t\t",_y_int[:16])
    print(60 * '_')
    AP = accuracy_score(decimal_labels,_y_int)
    RI = adjusted_rand_score(decimal_labels,_y_int)
    print("Accuracy (PURITY):" , AP)
    print("Accuracy (RAND INDEX):" , RI)
    return AP, RI
#+END_SRC

Finally combination of these functions are called in pipeline function.
#+BEGIN_SRC python
def pipeline(lib="torch", cluster_count_max=300, coefficient=2):
    cluster_count = len(np.unique(_y_int))
    result = []
    if lib == "torch":
        while cluster_count <= cluster_count_max:
            print(10 * "*" + "TRYING WITH " + str(cluster_count) + 10 * "*")
            init_clustring_torch(cluster_count)
            AP, RI = test_accuracy_torch()
            result.append([cluster_count, AP, RI])
            cluster_count *= coefficient
            cluster_count = int(cluster_count)
    elif lib == "scikit":
        while cluster_count <= cluster_count_max:
            print(10 * "*" + "TRYING WITH " + str(cluster_count) + 10 * "*")
            init_clustring_scikit(cluster_count)
            AP, RI = test_accuracy_scikit()
            result.append([cluster_count, AP, RI])
            cluster_count *= coefficient
            cluster_count = int(cluster_count)
    else:
        print("LIB NOT SUPPORTED")

    print(tabulate(result, headers=['K', 'AP', 'RI']))
#+END_SRC
** Result
In K-Means there isn't much to play with, the only variable that we can play with is K, and as it is expected the more clusters we have the more accurate desions are made by the algorithm. Of course by increasing K we will have multiple clusters representing one number but I think up to a point it can be tolerated. There is a table representing relation between K value and scores:
+-----+--------+--------+
|  K  | AP     | RI     |
+-----+--------+--------+
| 10  |0.597183|0.413258|
+-----+--------+--------+
| 12  |0.604917|0.430174|
+-----+--------+--------+
| 14  |0.65535 |0.472108|
+-----+--------+--------+
| 16  |0.680167|0.505252|
+-----+--------+--------+
| 19  |0.703983|0.529956|
+-----+--------+--------+
| 22  | 0.7115 |0.543603|
+-----+--------+--------+
| 26  |0.732617|0.552085|
+-----+--------+--------+
| 31  | 0.7495 |0.587359|
+-----+--------+--------+
| 37  |0.787217|0.62724 |
+-----+--------+--------+
| 44  | 0.8023 |0.651268|
+-----+--------+--------+
| 52  |0.82465 |0.676564|
+-----+--------+--------+
| 62  | 0.8436 |0.702975|
+-----+--------+--------+
| 74  |0.852117|0.721181|
+-----+--------+--------+
| 88  |0.865433|0.741598|
+-----+--------+--------+
| 105 |0.87975 |0.76377 |
+-----+--------+--------+
| 126 |0.880133|0.764852|
+-----+--------+--------+
| 151 |0.888583|0.780502|
+-----+--------+--------+
| 181 |0.896067|0.790999|
+-----+--------+--------+

As it was expected accuracy increases along side K!
* MeanShift
Libraries used for running this algorithm are almost identical to what we saw in K-Means script, the only difference is the algorithm itself.
** How does it work?
This algorithm tries to find density! It actually tries to seek the maxima of a density function. There are a few kernels for calculating weight of nearby point such as flat and Gaussian.
Flat kernel is really simple, if the point is within a specific range, the result will be 1 otherwise 0.
Gaussian kernel uses and exponential equation for weighting, scikit uses a flat kernel in its implementation.
There is one catch to this algorithm that being limitation on the sample size, as it has complexity of O(T*n^2)-based on scikit doc-. On sample sizes greater than 10K it'll cause some problems, like taking to long to generate results, that's what I experienced while working on mnist dataset that's why I'm executing this script on a random mnist subset.
** Libraries
#+BEGIN_SRC python  :session :results value
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.metrics import accuracy_score, adjusted_rand_score
from tabulate import tabulate
from util import _x, _y_int
import math
import numpy as np
import random as rand
#+END_SRC

** Extracting labels and evaluation
The procedure is the same as what we saw in K-Means script, we classify clusters and extract their label, and then we use a pipeline function to run the algorithm multiple times with different variables.
There are 3 function responsible for running and metering the algorithm:
1. classify_clusters : finds the number that has with highest repetition in each cluster
2. init_clustring_scikit : slices the loaded dataset and executes the algorithm
3. test_accuracy_scikit : runs metrics on the results

#+BEGIN_SRC  python
def classify_clusters(l1, l2):
    ref_labels = {}
    for i in range(len(np.unique(l1))):
        index = np.where(l1 == i,1,0)
        temp = np.bincount(l2[index==1]).argmax()
        ref_labels[i] = temp
    decimal_labels = np.zeros(len(l1))
    for i in range(len(l1)):
        decimal_labels[i] = ref_labels[l1[i]]
    return decimal_labels

def init_clustring_scikit(bandwidth=2, slice_size=6000):
    global clustering
    indexes = np.random.choice(len(_x), size=slice_size, replace=False)
    clustering = MeanShift(bandwidth=bandwidth, n_jobs=12)
    clustering.fit(_x[indexes])
    return _y_int[indexes]

def test_accuracy_scikit(labels):
    global clustering
    decimal_labels = classify_clusters(clustering.labels_, labels)
    print("NUMBER OF CLUSTERS:", len(np.unique(clustering.labels_)))
    print("predicted labels:\t", decimal_labels[:16].astype('int'))
    print("true labels:\t\t", labels[:16])
    print(60 * '_')
    AP = accuracy_score(decimal_labels,labels)
    RI = adjusted_rand_score(decimal_labels,labels)
    print("Accuracy (PURITY):" , AP)
    print("Accuracy (RAND INDEX):" , RI)
    return AP, RI, len(np.unique(clustering.labels_))
#+END_SRC


** Results
As we run the algorithm with multiple bandwidths on slice of size 6000 in the pipeline we get interesting results. Here is pipeline function:

#+BEGIN_SRC python
def pipeline(bandwidth_max=100, coefficient=2):
    bandwidth = 2
    result = []
    AP = None
    RI = None
    while bandwidth <= bandwidth_max:
        print(10 * "*" + "TRYING WITH " + str(bandwidth) + 10 * "*")
        labels = iO(T*n^2)nit_clustring_scikit(bandwidth)
        AP, RI, n= test_accuracy_scikit(labels)
        result.append([bandwidth, AP, RI, n])
        bandwidth *= coefficient
        bandwidth = math.ceil(bandwidth)
    print(tabulate(result, headers=['BandWidth', 'AP', 'RI', 'Cluster Count']))
#+END_SRC

When bandwidth is too small the algorithm starts to detecting too many clusters and as a result the accuracy is close or equal to 100%, and when bandwidth starts growing -larger than the appropriate bandwidth- the algorithm starts putting everything in a single cluster and as a result awful accuracy.
Here is a table showing relation between BandWidth and purity, random index and number of clusters:

+---------+--------+---------+------------+
|BandWidth| AP     | RI      |ClusterCount|
+---------+--------+---------+------------+
|    2    |  1     |  1      |    5770    |
+---------+--------+---------+------------+
|    3    |0.9995  |0.998818 |    5416    |
+---------+--------+---------+------------+
|    4    |0.995167|0.988411 |    4767    |
+---------+--------+---------+------------+
|    5    | 0.9385 |0.877167 |    3032    |
+---------+--------+---------+------------+
|    6    |0.759333|0.516909 |    1178    |
+---------+--------+---------+------------+
|    8    |0.150667|0.0041183|     29     |
+---------+--------+---------+------------+
|   10    |0.115333|    0    |     1      |
+---------+--------+---------+------------+
|   12    |0.109667|    0    |     1      |
+---------+--------+---------+------------+
|   15    | 0.118  |    0    |     1      |
+---------+--------+---------+------------+
|   18    |0.113333|    0    |     1      |
+---------+--------+---------+------------+
|   22    | 0.113  |    0    |     1      |
+---------+--------+---------+------------+
|   27    |0.115833|    0    |     1      |
+---------+--------+---------+------------+
|   33    |0.109833|    0    |     1      |
+---------+--------+---------+------------+
|   40    | 0.113  |    0    |     1      |
+---------+--------+---------+------------+
|   48    |0.111167|    0    |     1      |
+---------+--------+---------+------------+
|   58    | 0.1105 |    0    |     1      |
+---------+--------+---------+------------+
|   70    |0.114667|    0    |     1      |
+---------+--------+---------+------------+
|   84    |0.114167|    0    |     1      |
+---------+--------+---------+------------+
* DBSCAN
** How does it work?
There are two parameters in this algorithm, epsilon and min-samples. epsilon is the maximum distance from a cluster center and min sample is the minimum number of points required for a cluster to form. After cluster creation each point in every cluster starts its own circle and starts looking for other points to form a cluster.

As functions and what they do is really similar to what we already had, I'll only explain things that differ.
** Libraries
#+BEGIN_SRC python
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, adjusted_rand_score
from sklearn.preprocessing import StandardScaler
from tabulate import tabulate
from util import _x, _y_int
import math
import numpy as np
import random as rand
#+END_SRC

** Extracting labels
Because of noise detection in this algorithm we have bunch of -1 in generated labels that's we have to assign them all to a cluster before classifying. The classify_clusters function is almost the same as what we already had with the addition of -1 handler.

#+BEGIN_SRC python
def classify_clusters(l1, l2):
    ref_labels = {}
    m = np.unique(l1).max() + 1
    for i in range(l1.size):
        if l1[i] == -1:
            l1[i] = m
    for i in range(len(np.unique(l1))):
        index = np.where(l1 == i,1,0)
        temp = np.bincount(l2[index==1])
        ref_labels[i] = temp.argmax()
    decimal_labels = np.zeros(len(l1))
    for i in range(len(l1)):
        decimal_labels[i] = ref_labels[l1[i]]
    return decimal_labels
#+END_SRC

** Init functions and Evaluation

#+BEGIN_SRC python
def init_clustring_scikit(epsilon=2, min_samples=2):
    global clustering, slice_size
    indexes = np.random.choice(len(_x), size=slice_size, replace=False)
    clustering = DBSCAN(eps=epsilon, min_samples=min_samples)
    scaler = StandardScaler()
    x_train = scaler.fit_transform(_x[indexes])
    clustering.fit(x_train)
    print(clustering.labels_)
    return _y_int[indexes]

def test_accuracy_scikit(labels):
    global clustering
    core_samples_mask = np.zeros_like(clustering.labels_, dtype=bool)
    core_samples_mask[clustering.core_sample_indices_] = True
    decimal_labels = classify_clusters(clustering.labels_, labels)
    print("predicted labels:\t", decimal_labels[:16].astype('int'))
    print("true labels:\t\t", labels[:16])
    print(60 * '_')
    AP = accuracy_score(decimal_labels,labels)
    RI = adjusted_rand_score(decimal_labels,labels)
    print("Accuracy (PURITY):" , AP)
    print("Accuracy (RAND INDEX):" , RI)
    return AP, RI, len(np.unique(clustering.labels_))

def pipeline(epsilon_max=50, min_samples_max=50, coefficient=2):
    epsilon = 1
    min_samples = 1
    result = []
    AP = None
    RI = None
    while epsilon <= epsilon_max:
        while min_samples <= min_samples_max:
            print(10 * "*" + "TRYING WITH " + str(epsilon) + " " + str(min_samples) + 10 * "*")
            labels = init_clustring_scikit(epsilon, min_samples)
            AP, RI, n= test_accuracy_scikit(labels)
            result.append([epsilon, min_samples, AP, RI, n])
            min_samples *= coefficient
            min_samples = math.ceil(min_samples)
        min_samples = 1
        epsilon *= coefficient
        epsilon = math.ceil(epsilon)
    print(tabulate(result, headers=['epsilon', 'min_samples', 'AP', 'RI', 'Cluster Count']))
#+END_SRC

** Result
I couldn't find a meaningful relation between variables, but a simple explanation is that when min_samples is equal to 1 the algorithm creates a cluster for each data point. Another explanation is that for big epsilons the algorithm puts almost every point in a single cluster.
Here is a table showing results:

+-------+-----------+--------+------------+------------+
|epsilon|min_samples| AP     | RI         |ClusterCount|
+-------+-----------+--------+------------+------------+
|   1   |     1     |  1     |  1         |    6000    |
+-------+-----------+--------+------------+------------+
|   1   |     2     |0.112   |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |     3     |0.1125  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |     4     |0.113   |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |     5     |0.110333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |     6     |0.115333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |     8     |0.106333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    10     |0.116667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    12     |0.110167|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    15     |0.113333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    18     |0.116667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    22     | 0.114  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    27     |0.108167|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    33     |0.108167|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    40     | 0.1125 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   1   |    48     |0.108833|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     1     |   1    |  1         |    6000    |
+-------+-----------+--------+------------+------------+
|   2   |     2     |0.111333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     3     |0.113667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     4     |0.111667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     5     | 0.1065 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     6     | 0.114  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |     8     | 0.1085 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    10     | 0.1065 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    12     |0.115667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    15     | 0.1115 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    18     |0.111667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    22     |0.113333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    27     |0.109833|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    33     |0.108667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    40     |0.113833|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   2   |    48     |0.110167|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |     1     |   1    |  1         |    5995    |
+-------+-----------+--------+------------+------------+
|   3   |     2     | 0.114  |  0         |     2      |
+-------+-----------+--------+------------+------------+
|   3   |     3     |0.108667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |     4     |0.116667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |     5     | 0.112  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |     6     |0.117667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |     8     | 0.116  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    10     | 0.1165 |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    12     |0.107167|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    15     | 0.112  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    18     |0.113667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    22     |0.113333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    27     |0.110333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    33     |0.115667|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    40     | 0.113  |  0         |     1      |
+-------+-----------+--------+------------+------------+
|   3   |    48     |0.108333|  0         |     1      |
+-------+-----------+--------+------------+------------+
|   4   |     1     |   1    |  1         |    5928    |
+-------+-----------+--------+------------+------------+
|   4   |     2     |0.118333|0.000126079 |     31     |
+-------+-----------+--------+------------+------------+
|   4   |     3     |0.115167|-0.000179338|     11     |
+-------+-----------+--------+------------+------------+
|   4   |     4     |0.114167|-9.28756e-05|     3      |
+-------+-----------+--------+------------+------------+
|   4   |     5     |0.114833|     0      |     3      |
+-------+-----------+--------+------------+------------+
|   4   |     6     |0.117833|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |     8     |0.113833|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    10     |  0.12  |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    12     |0.109167|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    15     | 0.114  |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    18     |0.113667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    22     |0.113667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    27     |0.108167|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    33     |0.110833|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    40     |0.110667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   4   |    48     |0.116833|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |     1     |   1    |     1      |    5747    |
+-------+-----------+--------+------------+------------+
|   5   |     2     | 0.1525 | 0.00715352 |     38     |
+-------+-----------+--------+------------+------------+
|   5   |     3     |0.145333| 0.00482054 |     18     |
+-------+-----------+--------+------------+------------+
|   5   |     4     |0.132833| 0.00174484 |     16     |
+-------+-----------+--------+------------+------------+
|   5   |     5     | 0.128  |0.000893047 |     11     |
+-------+-----------+--------+------------+------------+
|   5   |     6     |0.123333|3.99368e-06 |     9      |
+-------+-----------+--------+------------+------------+
|   5   |     8     | 0.121  |-0.000185251|     6      |
+-------+-----------+--------+------------+------------+
|   5   |    10     |0.114333|     0      |     2      |
+-------+-----------+--------+------------+------------+
|   5   |    12     |0.119833|     0      |     2      |
+-------+-----------+--------+------------+------------+
|   5   |    15     |0.116833|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    18     |0.118167|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    22     |0.108667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    27     | 0.113  |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    33     | 0.114  |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    40     |0.111667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   5   |    48     | 0.1075 |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   6   |     1     |   1    |     1      |    5523    |
+-------+-----------+--------+------------+------------+
|   6   |     2     |0.184333|  0.020394  |     21     |
+-------+-----------+--------+------------+------------+
|   6   |     3     |0.171833| 0.0164176  |     8      |
+-------+-----------+--------+------------+------------+
|   6   |     4     |0.171667| 0.0145337  |     7      |
+-------+-----------+--------+------------+------------+
|   6   |     5     | 0.171  | 0.0133903  |     4      |
+-------+-----------+--------+------------+------------+
|   6   |     6     | 0.1585 | 0.00887019 |     2      |
+-------+-----------+--------+------------+------------+
|   6   |     8     |0.151667| 0.00730897 |     2      |
+-------+-----------+--------+------------+------------+
|   6   |    10     |0.156333| 0.00624232 |     2      |
+-------+-----------+--------+------------+------------+
|   6   |    12     |0.147667| 0.00478028 |     2      |
+-------+-----------+--------+------------+------------+
|   6   |    15     |  0.12  |0.000209748 |     4      |
+-------+-----------+--------+------------+------------+
|   6   |    18     | 0.1275 |0.000406582 |     4      |
+-------+-----------+--------+------------+------------+
|   6   |    22     |0.111667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   6   |    27     | 0.112  |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   6   |    33     |0.107667|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   6   |    40     | 0.1175 |     0      |     1      |
+-------+-----------+--------+------------+------------+
|   6   |    48     |0.114167|     0      |     1      |
+-------+-----------+--------+------------+------------+
|   8   |     1     |0.999667|   0.9992   |    5404    |
+-------+-----------+--------+------------+------------+
|   8   |     2     |0.217167| 0.0413145  |     28     |
+-------+-----------+--------+------------+------------+
|   8   |     3     |0.205333| 0.0374995  |     3      |
+-------+-----------+--------+------------+------------+
|   8   |     4     |0.201667| 0.0353645  |     4      |
+-------+-----------+--------+------------+------------+
|   8   |     5     | 0.1965 | 0.0338591  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |     6     |0.200333| 0.0334593  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |     8     |0.198833| 0.0328506  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    10     |0.196667| 0.0316348  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    12     | 0.1915 | 0.0311473  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    15     | 0.1965 | 0.0309658  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    18     |0.188833| 0.0283931  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    22     |0.185667| 0.0233562  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    27     | 0.189  | 0.0249011  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    33     | 0.187  | 0.0206931  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    40     | 0.185  | 0.0177916  |     2      |
+-------+-----------+--------+------------+------------+
|   8   |    48     |0.164167| 0.0127053  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |     1     | 0.9985 |  0.996546  |    5151    |
+-------+-----------+--------+------------+------------+
|  10   |     2     |0.258667| 0.0480627  |    118     |
+-------+-----------+--------+------------+------------+
|  10   |     3     |0.233167| 0.0458787  |     26     |
+-------+-----------+--------+------------+------------+
|  10   |     4     |0.217667| 0.0430279  |     11     |
+-------+-----------+--------+------------+------------+
|  10   |     5     |0.209333| 0.0420634  |     8      |
+-------+-----------+--------+------------+------------+
|  10   |     6     |0.205333| 0.0423173  |     3      |
+-------+-----------+--------+------------+------------+
|  10   |     8     |0.204667| 0.0427908  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    10     |0.212167| 0.0454559  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    12     | 0.207  | 0.0425168  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    15     |0.208333| 0.0422536  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    18     |0.208667| 0.0443901  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    22     | 0.205  | 0.0386904  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    27     |0.205167| 0.0407945  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    33     | 0.2005 | 0.0386994  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    40     |0.205667| 0.0396743  |     2      |
+-------+-----------+--------+------------+------------+
|  10   |    48     | 0.203  | 0.0379674  |     2      |
+-------+-----------+--------+------------+------------+
|  12   |     1     | 0.898  |  0.760749  |    4395    |
+-------+-----------+--------+------------+------------+
|  12   |     2     | 0.296  | 0.0646981  |    169     |
+-------+-----------+--------+------------+------------+
|  12   |     3     |0.261667| 0.0610819  |     65     |
+-------+-----------+--------+------------+------------+
|  12   |     4     |0.241667| 0.0546563  |     21     |
+-------+-----------+--------+------------+------------+
|  12   |     5     |0.224333| 0.0505364  |     18     |
+-------+-----------+--------+------------+------------+
|  12   |     6     |0.223333| 0.0504102  |     14     |
+-------+-----------+--------+------------+------------+
|  12   |     8     |0.224167| 0.0473934  |     11     |
+-------+-----------+--------+------------+------------+
|  12   |    10     |0.212667| 0.0458124  |     4      |
+-------+-----------+--------+------------+------------+
|  12   |    12     | 0.214  | 0.0449376  |     4      |
+-------+-----------+--------+------------+------------+
|  12   |    15     | 0.2145 |  0.045794  |     3      |
+-------+-----------+--------+------------+------------+
|  12   |    18     |0.218167|  0.046506  |     4      |
+-------+-----------+--------+------------+------------+
|  12   |    22     | 0.2115 |  0.045121  |     3      |
+-------+-----------+--------+------------+------------+
|  12   |    27     |0.212833| 0.0481875  |     2      |
+-------+-----------+--------+------------+------------+
|  12   |    33     |0.206333| 0.0452869  |     2      |
+-------+-----------+--------+------------+------------+
|  12   |    40     |0.211667| 0.0455116  |     2      |
+-------+-----------+--------+------------+------------+
|  12   |    48     |0.212667| 0.0458467  |     2      |
+-------+-----------+--------+------------+------------+
|  15   |     1     | 0.615  |  0.219514  |    2852    |
+-------+-----------+--------+------------+------------+
|  15   |     2     |0.237167| 0.0463517  |    114     |
+-------+-----------+--------+------------+------------+
|  15   |     3     |0.215167| 0.0518122  |     32     |
+-------+-----------+--------+------------+------------+
|  15   |     4     |0.219667| 0.0566393  |     18     |
+-------+-----------+--------+------------+------------+
|  15   |     5     |0.208333| 0.0494388  |     6      |
+-------+-----------+--------+------------+------------+
|  15   |     6     |0.211333|  0.05032   |     8      |
+-------+-----------+--------+------------+------------+
|  15   |     8     | 0.2115 | 0.0531746  |     5      |
+-------+-----------+--------+------------+------------+
|  15   |    10     |0.209167| 0.0570896  |     4      |
+-------+-----------+--------+------------+------------+
|  15   |    12     | 0.217  | 0.0584442  |     6      |
+-------+-----------+--------+------------+------------+
|  15   |    15     | 0.2255 | 0.0661595  |     3      |
+-------+-----------+--------+------------+------------+
|  15   |    18     |0.228833| 0.0678252  |     3      |
+-------+-----------+--------+------------+------------+
|  15   |    22     |0.230833| 0.0641389  |     4      |
+-------+-----------+--------+------------+------------+
|  15   |    27     |0.210833| 0.0634284  |     3      |
+-------+-----------+--------+------------+------------+
|  15   |    33     |0.215833| 0.0649979  |     3      |
+-------+-----------+--------+------------+------------+
|  15   |    40     |0.207167| 0.0589075  |     2      |
+-------+-----------+--------+------------+------------+
|  15   |    48     |0.214167| 0.0620427  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |     1     | 0.4155 | 0.0627408  |    1707    |
+-------+-----------+--------+------------+------------+
|  18   |     2     |0.204833| 0.0245047  |     77     |
+-------+-----------+--------+------------+------------+
|  18   |     3     |0.198833| 0.0246167  |     29     |
+-------+-----------+--------+------------+------------+
|  18   |     4     |0.190833| 0.0259082  |     12     |
+-------+-----------+--------+------------+------------+
|  18   |     5     |0.197833| 0.0297552  |     9      |
+-------+-----------+--------+------------+------------+
|  18   |     6     |0.186833| 0.0262412  |     4      |
+-------+-----------+--------+------------+------------+
|  18   |     8     | 0.189  | 0.0309404  |     3      |
+-------+-----------+--------+------------+------------+
|  18   |    10     |0.186667| 0.0296304  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    12     |0.191667| 0.0315301  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    15     |0.195333| 0.0373923  |     3      |
+-------+-----------+--------+------------+------------+
|  18   |    18     |0.192833| 0.0344452  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    22     | 0.1975 | 0.0371928  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    27     |0.191333| 0.0373274  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    33     |0.196333| 0.0417768  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    40     |0.194833| 0.0389586  |     2      |
+-------+-----------+--------+------------+------------+
|  18   |    48     |0.187167| 0.0434923  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |     1     | 0.2495 |  0.011196  |    762     |
+-------+-----------+--------+------------+------------+
|  22   |     2     |0.161333| 0.00496978 |     48     |
+-------+-----------+--------+------------+------------+
|  22   |     3     |0.160333| 0.00843873 |     9      |
+-------+-----------+--------+------------+------------+
|  22   |     4     |0.152667| 0.00601804 |     7      |
+-------+-----------+--------+------------+------------+
|  22   |     5     |0.156167| 0.00794966 |     3      |
+-------+-----------+--------+------------+------------+
|  22   |     6     |0.142167| 0.00646844 |     3      |
+-------+-----------+--------+------------+------------+
|  22   |     8     |0.154833| 0.00835903 |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    10     | 0.164  | 0.0108385  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    12     |0.170167| 0.0110854  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    15     |0.160333| 0.0119862  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    18     | 0.1645 | 0.0104256  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    22     |0.171333| 0.0117579  |     3      |
+-------+-----------+--------+------------+------------+
|  22   |    27     |0.166333|  0.013779  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    33     |0.158667| 0.0123178  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    40     |0.163833| 0.0114593  |     2      |
+-------+-----------+--------+------------+------------+
|  22   |    48     |0.166333| 0.0145922  |     2      |
+-------+-----------+--------+------------+------------+
|  27   |     1     |0.185333| 0.00370312 |    377     |
+-------+-----------+--------+------------+------------+
|  27   |     2     |0.130667|0.000727253 |     35     |
+-------+-----------+--------+------------+------------+
|  27   |     3     |0.135167| 0.00135523 |     14     |
+-------+-----------+--------+------------+------------+
|  27   |     4     | 0.126  | 0.00114075 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |     5     |0.123167| 0.00121083 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |     6     |0.125833| 0.0013828  |     3      |
+-------+-----------+--------+------------+------------+
|  27   |     8     |0.124333|0.000616473 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    10     |0.129167| 0.00195021 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    12     | 0.122  |0.000648059 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    15     |0.123333| 0.00132483 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    18     |0.135333| 0.00251873 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    22     |0.132167| 0.00173575 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    27     | 0.1275 | 0.00182336 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    33     | 0.1325 | 0.00197115 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    40     |0.137833| 0.00263512 |     2      |
+-------+-----------+--------+------------+------------+
|  27   |    48     | 0.1275 | 0.00249718 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |     1     | 0.138  |0.000563966 |    160     |
+-------+-----------+--------+------------+------------+
|  33   |     2     |0.127667|0.000388157 |     18     |
+-------+-----------+--------+------------+------------+
|  33   |     3     |0.121833|5.94815e-05 |     11     |
+-------+-----------+--------+------------+------------+
|  33   |     4     |0.119667|0.000236978 |     5      |
+-------+-----------+--------+------------+------------+
|  33   |     5     |0.117333|0.000256806 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |     6     | 0.1155 |8.42259e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |     8     |0.126167|0.000796342 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    10     |0.123167|0.000388536 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    12     | 0.119  |0.000463346 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    15     |0.119667|0.000533426 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    18     |  0.12  |0.000399171 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    22     | 0.121  |0.000251038 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    27     |0.122667|0.000521123 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    33     |0.122167|0.000507524 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    40     | 0.1205 |0.000342103 |     2      |
+-------+-----------+--------+------------+------------+
|  33   |    48     |0.116167|0.000692975 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |     1     |0.128833|0.000189544 |     89     |
+-------+-----------+--------+------------+------------+
|  40   |     2     |0.122667|9.79239e-05 |     15     |
+-------+-----------+--------+------------+------------+
|  40   |     3     |0.112833|0.000152592 |     3      |
+-------+-----------+--------+------------+------------+
|  40   |     4     |0.113667|0.000122208 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |     5     |  0.11  |1.29211e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |     6     |0.117833|0.000120968 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |     8     |0.110833|1.85982e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    10     |0.115833|0.000290297 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    12     | 0.1185 |0.000155974 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    15     |0.118667| 9.0435e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    18     | 0.127  | 7.1668e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    22     |0.120667| 8.5268e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    27     |0.112167|0.000103917 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    33     | 0.1225 |7.65653e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    40     |0.117833|9.48118e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  40   |    48     | 0.1205 |5.93216e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |     1     |0.127167|0.000153136 |     63     |
+-------+-----------+--------+------------+------------+
|  48   |     2     | 0.1145 |4.45421e-05 |     9      |
+-------+-----------+--------+------------+------------+
|  48   |     3     |0.113667|-9.01441e-06|     2      |
+-------+-----------+--------+------------+------------+
|  48   |     4     |0.116167|7.09657e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |     5     |0.120667|0.000178664 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |     6     |0.119333|5.50319e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |     8     | 0.1095 |-4.89426e-05|     2      |
+-------+-----------+--------+------------+------------+
|  48   |    10     |0.119333|2.36379e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    12     |0.119167|4.37861e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    15     | 0.115  |4.41703e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    18     | 0.1115 |-4.02694e-05|     2      |
+-------+-----------+--------+------------+------------+
|  48   |    22     |0.113333|7.52591e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    27     |0.117833|0.000125597 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    33     |0.114167|-8.65388e-06|     2      |
+-------+-----------+--------+------------+------------+
|  48   |    40     | 0.1195 |3.94606e-05 |     2      |
+-------+-----------+--------+------------+------------+
|  48   |    48     | 0.118  |6.77043e-05 |     2      |
+-------+-----------+--------+------------+------------+
* Agglomerative
** How does it work?
This algorithm forms a tree and starts from the leaves. First of all it puts the most similar points in a cluster, then puts similar clusters in clusters and so on until we have one giant cluster! So we'll have a hierarchy as a result.
Also there isn't much to play with in this algorithm. Although we can provide number of clusters but results are clear and it's pointless, the more clusters we have the higher accuracy we'll get out of this algorithm. That's why I'll provide table for different linkages only.
** Libraries
#+BEGIN_SRC python
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import accuracy_score, adjusted_rand_score
from tabulate import tabulate
from util import _x, _y_int
import math
import numpy as np
import random as rand
#+END_SRC
** Init functions and evaluation
It's almost identical to what we had in mentioned algorithms, so there isn't much to explain.
#+BEGIN_SRC python
clustering = None

def classify_clusters(l1, l2):
    ref_labels = {}
    for i in range(len(np.unique(l1))):
        index = np.where(l1 == i,1,0)
        temp = np.bincount(l2[index==1]).argmax()
        ref_labels[i] = temp
    decimal_labels = np.zeros(len(l1))
    for i in range(len(l1)):
        decimal_labels[i] = ref_labels[l1[i]]
    return decimal_labels

def init_clustring_scikit(linkage, slice_size=10000):
    global clustering
    indexes = np.random.choice(len(_x), size=slice_size, replace=False)
    clustering = AgglomerativeClustering(n_clusters=10, affinity="euclidean", compute_full_tree=True, linkage=linkage)
    clustering.fit(_x[indexes])
    return _y_int[indexes]

def test_accuracy_scikit(labels):
    global clustering
    decimal_labels = classify_clusters(clustering.labels_, labels)
    print("predicted labels:\t", decimal_labels[:16].astype('int'))
    print("true labels:\t\t", labels[:16])
    print(60 * '_')
    AP = accuracy_score(decimal_labels,labels)
    RI = adjusted_rand_score(decimal_labels,labels)
    print("Accuracy (PURITY):" , AP)
    print("Accuracy (RAND INDEX):" , RI)
    return AP, RI

def pipeline(linkage=["ward", "single", "average", "complete"]):
    result = []
    AP = None
    RI = None
    for x in linkage:
        print(10 * "*" + "TRYING WITH " + x + 10 * "*")
        labels = init_clustring_scikit(x)
        AP, RI = test_accuracy_scikit(labels)
        result.append([x, AP, RI])
    print(tabulate(result, headers=['linkage', 'AP', 'RI']))
#+END_SRC

#+RESULTS:
: None

** Result
This table is really self explanatory, ward has the best score and single has the worst score.

+--------+------+----------+
|linkage | AP   | RI       |
+--------+------+----------+
| ward   |0.6321|0.471757  |
+--------+------+----------+
|single  |0.1113|1.2206e-05|
+--------+------+----------+
|average |0.2045|0.0420506 |
+--------+------+----------+
|complete|0.3879| 0.204411 |
+--------+------+----------+
